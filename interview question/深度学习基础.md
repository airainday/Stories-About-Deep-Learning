## 1. ResNet中的残差块解决了什么问题？

不断加深网络深度会不仅会导致梯度爆炸和梯度消失问题的加剧，并且可能会出现网络退化（深层网络效果在训练和测试集上都不如浅层网络好）的情况。理论上来说，网络的加深即使不会提升模型精度但也不至于让模型变差，但事实上由于非线性激活函数的存在，会造成很多不可逆的信息损失，网络加深到一定程度，过多的信息损失就会造成网络退化。

![image-20240222224502989](https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240222224502989.png)

**而ResNet提出了残差块，提供恒等映射功能，可以让模型起码不会差于浅层网络。**残差结构（如上图所示）的目的是，**随着网络的加深，使 F(x) 逼近于0**，使得深度网络的精度在最优浅层网络的基础上不会下降。看到这里你或许会有疑问，既然如此为什么不直接选取最优的浅层网络呢？这是因为最优的浅层网络结构并不易找寻（你不知道多少层是最优的，难道要每个任务都一层一层的尝试？），而ResNet可以通过增加深度，找到最优的浅层网络并保证深层网络不会因为层数的叠加而发生网络退化。

## 2. 深度学习中梯度消失和梯度爆炸问题

神经网络参数更新依靠损失函数对该参数的梯度，在求梯度时，最底层的网络参数的梯度更容易出现梯度消失或梯度爆炸问题。比如下面一个例子（假设每层只有一个神经元(不考虑偏置项)，且激活函数为sigmoid）：
$$
z_{i+1}=w_ia_i \\
a_{i+1}=\sigma(z_{i+1})
$$
$$a_i$$为每层输入，$$w_i$$​和为每层权重参数和偏置

![image-20240225212126096](https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240225212126096.png)

输出对第一层权重$$w_0$$的导数计算如下：
$$
\frac{\partial a_3}{\partial w_0}=\frac{\partial a_3}{\partial z_3}\frac{\partial z_3}{\partial a_2}\frac{\partial a_2}{\partial z_2}\frac{\partial z_2}{\partial a_1}\frac{\partial a_1}{\partial z_1}\frac{\partial z_1}{\partial w_0}=\sigma^{,}(z_3)w_2\sigma^{,}(z_2)w_1\sigma^{,}(z_1)a_0
$$
可以看到第一层的导数与**输入和后面每一层网络的权重及激活函数相关**。

由于sigmoid激活函数最大值为0.25，因此随着网络层数的加深，第一层参数的梯度会越来越小，形成**梯度消失**。

同样，当每一层网络参数权重初始化为一个较大值时，虽然和激活函数的导数相乘会减小这个值，但是随着神经网络的加深，梯度呈指数级增长（如果还是大于1），就会引发梯度爆炸。但是从AlexNet开始，神经网络中就**使用ReLU函数替换了Sigmoid（可以解决梯度消失问题）**，同时BN（Batch Normalization)层的加入（让$$z_i$$处于一个合理值，从而使激活函数的导数值合理），也基本解决了梯度消失/爆炸问题。（==有一个疑问，虽然通过RELU和BN层可以控制激活函数导数的取值了，但是每一层的权重还是会影响梯度，如何控制每一层权重大小呢？？？==）
