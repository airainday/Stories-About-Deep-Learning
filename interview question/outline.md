## 1. ResNet中的残差块解决了什么问题？

不断加深网络深度会不仅会导致梯度爆炸和梯度消失问题的加剧，并且可能会出现网络退化（深层网络效果在训练和测试集上都不如浅层网络好）的情况。理论上来说，网络的加深即使不会提升模型精度但也不至于让模型变差，但事实上由于非线性激活函数的存在，会造成很多不可逆的信息损失，网络加深到一定程度，过多的信息损失就会造成网络退化。

![image-20240222224502989](https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240222224502989.png)

**而ResNet提出了残差块，提供恒等映射功能，可以让模型起码不会差于浅层网络。**残差结构（如上图所示）的目的是，**随着网络的加深，使 F(x) 逼近于0**，使得深度网络的精度在最优浅层网络的基础上不会下降。看到这里你或许会有疑问，既然如此为什么不直接选取最优的浅层网络呢？这是因为最优的浅层网络结构并不易找寻（你不知道多少层是最优的，难道要每个任务都一层一层的尝试？），而ResNet可以通过增加深度，找到最优的浅层网络并保证深层网络不会因为层数的叠加而发生网络退化。

## 2. 深度学习中梯度消失和梯度爆炸问题



