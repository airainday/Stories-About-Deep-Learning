## 简述yolov5网络

yolov5是基于锚框单阶段目标检测模型，主要由三部分构成：1.用于特征提取的backbone网络（CSPDarkNet53），主要由类似ResNet残差块的C3模块以及SPPF模块组成；2.用于融合不同层级特征的neck网络（PAN组件）；3.用于输出预测框的检测头，默认为3个，下采样倍数分别为8,16,32，大特征图用于检测小目标。

最后经过NMS删除重复且质量不高的检测框，得到最终的输出。

关于更详细的损失计算、评价指标、数据增强、训练策略可以参考Obsdian中OD板块的yolov5 Canvas。

## FPN组件

特征金字塔网络中提出的FPN组件，主要是为了解决小物体在下采样过程中信息损失的问题，通过将不同尺度的特征图进行融合（元素相加）来使特征图既包含较强的语义信息（深层特征图）也包含较多的边缘纹理细节信息（浅层特征图）。**需要注意的是这里特征图融合是元素相加，而yolov5中的PAN模块的特征图融合是concat，通道维度拼接。**

<img src="https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240301222134179.png" alt="image-20240301222134179" style="zoom: 67%;" />

FPN网络的具体结构如下：需要注意的是P2~P4这几个检测头（全连接层进行分类和回归）的参数是共享的

<img src="https://pic3.zhimg.com/v2-fe85fb352b9c212fb6d5416330fad9d2_r.jpg" style="zoom:67%;" />

## PAN组件

PAN是yolov5的neck部分，用于进行深层特征图和浅层特征图的融合，是FPN的改进版，FPN只包含自顶向底的融合，而PAN又增加了自底向顶的融合：

![image-20240305214635936](https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240305214635936.png)

## 非极大抑制NMS

参考链接：[Work-Record/05 OD/md/非极大值抑制.md at master · airainday/Work-Record (github.com)](https://github.com/airainday/Work-Record/blob/master/05 OD/md/非极大值抑制.md)

NMS主要是为了筛选掉同一目标质量低的重复检测框，有两个阈值IOU阈值和Conf阈值，经过Conf阈值先过滤掉质量差的检测框，然后从剩余框中选出置信度分数最高的框，遍历剩下的框，将与其IOU大于IOU阈值的框过滤掉。然后再从剩下的框中选出分数最高的框，重复上述步骤，直到所有框都没了，你每次选出的分数最高的框就是最终结果。

## yolov5 SE模块

参考链接：[优化改进YOLOv5算法之添加SE、CBAM、CA模块(超详细)_yolov5 添加msenet模块-CSDN博客](https://blog.csdn.net/qq_40716944/article/details/128525201?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~PaidSort-1-128525201-blog-129235560.235^v43^pc_blog_bottom_relevance_base2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~PaidSort-1-128525201-blog-129235560.235^v43^pc_blog_bottom_relevance_base2&utm_relevant_index=2)

SE模块来自于SENet网络，创新点在于关注channel之间的关系，希望模型可以自动学习到不同channel特征的重要程度。为此，SENet提出了Squeeze-and-Excitation (SE)模块：

其中步骤2是将特征图进行全局平均或者最大池化，得到1x1xC的特征图，然后步骤3是将上一步得到的向量经过MLP网络（FC+Relu+FC+Sigmoid），这样就得到了每个通道的权重，这些权重作为重要程度系数乘以步骤1对应的特征图。（可以增强或者削弱某些特征图的强度，有点注意力机制的思想）。

![img](https://img-blog.csdnimg.cn/ab2346d213004bc2ae2d25f6d38536ba.png)

## 感受野计算

感受野（Receptive Field）是特征图上某个点能看到的输入图像上的区域。

其中l+1层的感受野计算公式如下：

$$RF_{l+1}=RF_l+(k-1)S_l；S_l=Stride_1*Stride_2*...*Stride_l$$

其中k为l+1层的核尺寸，S_l为前l层的步长之积，注意当前层的步长并不影响当前层的感受野。

## yolov5添加解耦头

解耦头将分类和定位预测解耦（原来是放在一起预测，一个锚框输出num_cls+4+1个参数）。

==三个特征图的解耦头参数是否共用？？？问问文钊。==

![image-20240305222755056](https://cdn.jsdelivr.net/gh/airainday/blogimage@main/image-20240305222755056.png)





